{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33db0745-1ab5-4d91-a198-f3e2e8f34718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 1. Imports & Setup\n",
    "# =====================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "import jiwer\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "067c9cb6-c352-4283-90a0-6e98ae763f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mild-snowflake-1</strong> at: <a href='https://wandb.ai/nimbus-neuron/asr-rnn/runs/0vrl649u' target=\"_blank\">https://wandb.ai/nimbus-neuron/asr-rnn/runs/0vrl649u</a><br> View project at: <a href='https://wandb.ai/nimbus-neuron/asr-rnn' target=\"_blank\">https://wandb.ai/nimbus-neuron/asr-rnn</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251006_141110-0vrl649u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/innocentchikwanda/Desktop/Dev/DeepML/DeepLearningICS/Prosit2/wandb/run-20251006_151415-c9xz3vn6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nimbus-neuron/asr-rnn-sweep/runs/c9xz3vn6' target=\"_blank\">clear-haze-1</a></strong> to <a href='https://wandb.ai/nimbus-neuron/asr-rnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nimbus-neuron/asr-rnn-sweep' target=\"_blank\">https://wandb.ai/nimbus-neuron/asr-rnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nimbus-neuron/asr-rnn-sweep/runs/c9xz3vn6' target=\"_blank\">https://wandb.ai/nimbus-neuron/asr-rnn-sweep/runs/c9xz3vn6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 2. Config (for sweep compatibility)\n",
    "# =====================================================\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--project\", type=str, default=\"asr-rnn-sweep\")\n",
    "parser.add_argument(\"--accent\", type=str, default=\"shona\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=2)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=4)\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "parser.add_argument(\"--hidden_dim\", type=int, default=256)\n",
    "parser.add_argument(\"--num_layers\", type=int, default=2)\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "parser.add_argument(\"--max_train_samples\", type=int, default=200)\n",
    "parser.add_argument(\"--max_val_samples\", type=int, default=50)\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "wandb.init(project=args.project, config=vars(args))\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a39220a9-3c49-4d48-812e-d756f8b496a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2e6558d422462a8e33223af7eefbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713e667b7de64b688b5afa4e03925371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "afrispeech-200.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset scripts are no longer supported, but found afrispeech-200.py",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# =====================================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 3. Load Real Dataset (AfriSpeech subset)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# =====================================================\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtobiolatunji/afrispeech-200\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m train_set \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mmax_train_samples))\n\u001b[1;32m      6\u001b[0m val_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dataset \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/datasets/load.py:1392\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   1388\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1389\u001b[0m )\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/datasets/load.py:1132\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     features \u001b[38;5;241m=\u001b[39m _fix_for_backward_compatible_features(features)\n\u001b[0;32m-> 1132\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/datasets/load.py:1031\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1027\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1028\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1029\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1030\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1031\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/datasets/load.py:989\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     api\u001b[38;5;241m.\u001b[39mhf_hub_download(\n\u001b[1;32m    983\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m    984\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[1;32m    988\u001b[0m     )\n\u001b[0;32m--> 989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset scripts are no longer supported, but found afrispeech-200.py"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 3. Load Real Dataset (AfriSpeech subset)\n",
    "# =====================================================\n",
    "dataset = load_dataset(\"tobiolatunji/afrispeech-200\", config.accent)\n",
    "train_set = dataset[\"train\"].select(range(config.max_train_samples))\n",
    "val_split = \"dev\" if \"dev\" in dataset else \"test\"\n",
    "val_set = dataset[val_split].select(range(config.max_val_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2c1468-a874-49ba-beb3-1867a47ae1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 4. Feature Extraction & Tokenization\n",
    "# =====================================================\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "\n",
    "sample_rate = 16000\n",
    "n_mels = 80\n",
    "\n",
    "mel_transform = MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels)\n",
    "\n",
    "# Build simple tokenizer from training transcripts\n",
    "chars = sorted(list(set(\"\".join(train_set[\"sentence\"]).lower())))\n",
    "char2idx = {c: i+1 for i, c in enumerate(chars)}  # reserve 0 for blank\n",
    "idx2char = {i: c for c, i in char2idx.items()}\n",
    "vocab_size = len(char2idx) + 1\n",
    "\n",
    "def encode_text(text):\n",
    "    return torch.tensor([char2idx.get(c, 0) for c in text.lower() if c in char2idx], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    specs, labels, input_lengths, label_lengths = [], [], [], []\n",
    "    for b in batch:\n",
    "        wav = torch.tensor(b[\"audio\"][\"array\"]).float()\n",
    "        if b[\"audio\"][\"sampling_rate\"] != sample_rate:\n",
    "            wav = Resample(b[\"audio\"][\"sampling_rate\"], sample_rate)(wav)\n",
    "        mel = mel_transform(wav).transpose(0, 1)\n",
    "        specs.append(mel)\n",
    "        label = encode_text(b[\"sentence\"])\n",
    "        labels.append(label)\n",
    "        input_lengths.append(mel.shape[0])\n",
    "        label_lengths.append(len(label))\n",
    "    specs = nn.utils.rnn.pad_sequence(specs, batch_first=True)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "    return specs, labels, torch.tensor(input_lengths), torch.tensor(label_lengths)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# =====================================================\n",
    "# 5. Basic RNN Model (CTC-compatible)\n",
    "# =====================================================\n",
    "class BasicRNNASR(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_dim, hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # bidirectional doubles dim\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc(out)\n",
    "\n",
    "model = BasicRNNASR(n_mels, config.hidden_dim, vocab_size, config.num_layers, config.dropout).to(device)\n",
    "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "# =====================================================\n",
    "# 6. Training & Validation Loops\n",
    "# =====================================================\n",
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for specs, labels, input_lens, label_lens in tqdm(train_loader, desc=\"Train\"):\n",
    "        specs, labels = specs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(specs)  # (B, T, vocab)\n",
    "        log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "        loss = criterion(log_probs.transpose(0, 1), labels, input_lens, label_lens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        wandb.log({\"train_batch_loss\": loss.item()})\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss, wers = 0, []\n",
    "    with torch.no_grad():\n",
    "        for specs, labels, input_lens, label_lens in tqdm(val_loader, desc=\"Val\"):\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            logits = model(specs)\n",
    "            log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "            loss = criterion(log_probs.transpose(0, 1), labels, input_lens, label_lens)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Decode predictions\n",
    "            pred_ids = torch.argmax(log_probs, dim=-1).squeeze().detach().cpu().numpy()\n",
    "            decoded = \"\".join([idx2char[i] for i in np.unique(pred_ids) if i in idx2char])\n",
    "            true_text = \"\".join([idx2char[i.item()] for i in labels[0] if i.item() in idx2char])\n",
    "            wers.append(jiwer.wer(true_text, decoded))\n",
    "    return total_loss / len(val_loader), np.mean(wers)\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    train_loss = train_one_epoch()\n",
    "    val_loss, val_wer = evaluate()\n",
    "    wandb.log({\"epoch\": epoch + 1, \"train_loss\": train_loss, \"val_loss\": val_loss, \"val_wer\": val_wer})\n",
    "    print(f\"Epoch {epoch+1}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}, Val WER {val_wer:.3f}\")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# =====================================================\n",
    "# 7. Sweep Config (Small)\n",
    "# =====================================================\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_wer\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-3, 5e-4]},\n",
    "        \"hidden_dim\": {\"values\": [128, 256]},\n",
    "        \"num_layers\": {\"values\": [1, 2]},\n",
    "        \"dropout\": {\"values\": [0.1, 0.3]},\n",
    "        \"batch_size\": {\"values\": [4, 8]}\n",
    "    }\n",
    "}\n",
    "\n",
    "# =====================================================\n",
    "# 8. Sweep Launch (Manual Trigger)\n",
    "# =====================================================\n",
    "# Uncomment once ready:\n",
    "# sweep_id = wandb.sweep(sweep_config, project=args.project)\n",
    "# wandb.agent(sweep_id, function=lambda: None)  # placeholder; run manually in W&B dashboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
