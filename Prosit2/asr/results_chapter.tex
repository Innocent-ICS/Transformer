\chapter{Results}
\label{ch:results}

This chapter presents the experimental results from evaluating six encoder-decoder RNN architectures on the AfriSpeech-200 Shona dataset. All models were trained for 50 epochs using identical hyperparameters to ensure fair comparison.

\section{Experimental Configuration}
\label{sec:experimental-config}

Table~\ref{tab:config} summarizes the configuration used across all experiments.

\begin{table}[htbp]
\centering
\caption{Experimental configuration}
\label{tab:config}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Dataset & AfriSpeech-200 (Shona) \\
Audio Features & 40-dimensional MFCC \\
Sample Rate & 16kHz \\
Hidden Size & 128 \\
Number of Layers & 1 \\
Dropout & 0.3 \\
Batch Size & 8 \\
Learning Rate & 0.001 \\
Optimizer & Adam \\
Gradient Clipping & 5.0 \\
Epochs & 50 \\
Loss Function & CTC \\
\bottomrule
\end{tabular}
\end{table}

\section{Overall Performance Comparison}
\label{sec:overall-performance}

Table~\ref{tab:overall-results} presents the test set performance for all completed experiments.

\begin{table}[htbp]
\centering
\caption{Test set performance across all models}
\label{tab:overall-results}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{CTC Loss} & \textbf{Accuracy} & \textbf{Perplexity} & \textbf{CER} & \textbf{WER} \\
\midrule
Vanilla RNN & 28.09 & 0.09\% & 1.59$\times$10$^{12}$ & 98.89\% & 100\% \\
Vanilla RNN + Attn & 3.97 & 1.20\% & 53.20 & 96.02\% & 100\% \\
LSTM & -- & -- & -- & -- & -- \\
LSTM + Attn & -- & -- & -- & -- & -- \\
GRU & -- & -- & -- & -- & -- \\
GRU + Attn & -- & -- & -- & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\section{Vanilla RNN Results}
\label{sec:vanilla-rnn-results}

\subsection{Training Dynamics}
\label{subsec:vanilla-training}

The Vanilla RNN baseline model exhibited rapid initial learning followed by early plateau. Table~\ref{tab:vanilla-training} shows selected training epochs.

\begin{table}[htbp]
\centering
\caption{Vanilla RNN training progress (selected epochs)}
\label{tab:vanilla-training}
\begin{tabular}{ccc}
\toprule
\textbf{Epoch} & \textbf{Train Loss} & \textbf{Val Loss} \\
\midrule
1 & 28.50 & 18.33 \\
5 & 6.09 & 5.75 \\
10 & 5.83 & 5.66 \\
20 & 5.74 & 5.61 \\
30 & 5.51 & 5.62 \\
40 & 5.42 & 5.65 \\
50 & 5.31 & 5.63 \\
\midrule
\textbf{Reduction} & \textbf{81.4\%} & \textbf{69.3\%} \\
\bottomrule
\end{tabular}
\end{table}

The training loss decreased by 81.4\% from epoch 1 to epoch 50, while validation loss decreased by 69.3\%. Loss convergence occurred around epoch 10, with minimal improvement thereafter.

\subsection{Test Set Performance}
\label{subsec:vanilla-test}

Table~\ref{tab:vanilla-test} presents the final test set evaluation metrics.

\begin{table}[htbp]
\centering
\caption{Vanilla RNN test set results}
\label{tab:vanilla-test}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
CTC Loss & 28.09 \\
Accuracy & 0.09\% \\
Perplexity & 1.59$\times$10$^{12}$ \\
Character Error Rate & 98.89\% \\
Word Error Rate & 100.00\% \\
\bottomrule
\end{tabular}
\end{table}

\section{Vanilla RNN with Attention Results}
\label{sec:vanilla-attention-results}

\subsection{Training Dynamics}
\label{subsec:vanilla-attn-training}

The attention-augmented model demonstrated continuous improvement throughout training, with a notable performance jump after epoch 25. Table~\ref{tab:vanilla-attn-training} shows selected training epochs.

\begin{table}[htbp]
\centering
\caption{Vanilla RNN with attention training progress (selected epochs)}
\label{tab:vanilla-attn-training}
\begin{tabular}{ccc}
\toprule
\textbf{Epoch} & \textbf{Train Loss} & \textbf{Val Loss} \\
\midrule
1 & 16.93 & 7.90 \\
5 & 7.67 & 7.19 \\
10 & 7.36 & 7.09 \\
20 & 6.79 & 6.78 \\
25 & 7.16 & 5.35 \\
30 & 3.96 & 5.45 \\
40 & 3.56 & 4.58 \\
50 & 3.42 & 4.48 \\
\midrule
\textbf{Reduction} & \textbf{79.8\%} & \textbf{43.3\%} \\
\bottomrule
\end{tabular}
\end{table}

The training loss decreased by 79.8\% and validation loss by 43.3\%. A significant loss reduction occurred between epochs 25 and 30, indicating a breakthrough in learning.

\subsection{Test Set Performance}
\label{subsec:vanilla-attn-test}

Table~\ref{tab:vanilla-attn-test} presents the final test set evaluation metrics.

\begin{table}[htbp]
\centering
\caption{Vanilla RNN with attention test set results}
\label{tab:vanilla-attn-test}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
CTC Loss & 3.97 \\
Accuracy & 1.20\% \\
Perplexity & 53.20 \\
Character Error Rate & 96.02\% \\
Word Error Rate & 100.00\% \\
\bottomrule
\end{tabular}
\end{table}

\section{Attention Mechanism Impact}
\label{sec:attention-impact}

Table~\ref{tab:attention-comparison} quantifies the performance improvement achieved by adding the Bahdanau attention mechanism to the Vanilla RNN architecture.

\begin{table}[htbp]
\centering
\caption{Performance comparison: Vanilla RNN vs. Vanilla RNN with attention}
\label{tab:attention-comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Vanilla RNN} & \textbf{+ Attention} & \textbf{Improvement} \\
\midrule
Test CTC Loss & 28.09 & 3.97 & 85.9\% $\downarrow$ \\
Test Accuracy & 0.09\% & 1.20\% & 13.0$\times$ $\uparrow$ \\
Test Perplexity & 1.59$\times$10$^{12}$ & 53.20 & $>$99.9\% $\downarrow$ \\
Test CER & 98.89\% & 96.02\% & 2.9\% $\downarrow$ \\
Test WER & 100.00\% & 100.00\% & -- \\
Final Train Loss & 5.31 & 3.42 & 35.6\% $\downarrow$ \\
Final Val Loss & 5.63 & 4.48 & 20.4\% $\downarrow$ \\
\bottomrule
\end{tabular}
\end{table}

The attention mechanism yielded substantial improvements across all metrics except WER, where both models achieved 100\% error rate. Most notably, test accuracy increased by a factor of 13.0, and test CTC loss decreased by 85.9\%.

\section{Training Convergence Analysis}
\label{sec:convergence-analysis}

Figure~\ref{fig:convergence-comparison} would show the training and validation loss curves for both models. Key observations from the complete training logs:

\subsection{Vanilla RNN Convergence Pattern}
\begin{itemize}
    \item Rapid decrease: Epochs 1--5 (28.50 $\rightarrow$ 6.09)
    \item Plateau: Epochs 10--50 (5.83 $\rightarrow$ 5.31)
    \item Train-validation gap: 0.32 (minimal overfitting)
\end{itemize}

\subsection{Vanilla RNN with Attention Convergence Pattern}
\begin{itemize}
    \item Gradual decrease: Epochs 1--25 (16.93 $\rightarrow$ 7.16)
    \item Breakthrough: Epochs 25--30 (7.16 $\rightarrow$ 3.96)
    \item Continued improvement: Epochs 30--50 (3.96 $\rightarrow$ 3.42)
    \item Train-validation gap: 1.06 (moderate overfitting)
\end{itemize}

\section{Complete Training Logs}
\label{sec:complete-logs}

\subsection{Vanilla RNN: All 50 Epochs}
\label{subsec:vanilla-complete}

\begin{table}[htbp]
\centering
\caption{Complete training log: Vanilla RNN}
\label{tab:vanilla-complete}
\small
\begin{tabular}{ccc|ccc|ccc}
\toprule
\textbf{Ep} & \textbf{Train} & \textbf{Val} & \textbf{Ep} & \textbf{Train} & \textbf{Val} & \textbf{Ep} & \textbf{Train} & \textbf{Val} \\
\midrule
1 & 28.50 & 18.33 & 18 & 5.65 & 5.61 & 35 & 5.49 & 5.59 \\
2 & 14.70 & 9.12 & 19 & 5.64 & 5.58 & 36 & 5.43 & 5.57 \\
3 & 7.76 & 6.33 & 20 & 5.74 & 5.61 & 37 & 5.56 & 5.61 \\
4 & 6.44 & 5.85 & 21 & 5.68 & 5.59 & 38 & 5.55 & 5.64 \\
5 & 6.09 & 5.75 & 22 & 5.58 & 5.59 & 39 & 5.40 & 5.58 \\
6 & 6.02 & 5.75 & 23 & 5.70 & 5.57 & 40 & 5.42 & 5.65 \\
7 & 5.93 & 5.70 & 24 & 5.58 & 5.58 & 41 & 5.44 & 5.68 \\
8 & 5.90 & 5.70 & 25 & 5.70 & 5.58 & 42 & 5.31 & 5.62 \\
9 & 5.89 & 5.67 & 26 & 5.63 & 5.57 & 43 & 5.37 & 5.59 \\
10 & 5.83 & 5.66 & 27 & 5.55 & 5.59 & 44 & 5.38 & 5.63 \\
11 & 5.87 & 5.67 & 28 & 5.53 & 5.56 & 45 & 5.53 & 5.66 \\
12 & 5.77 & 5.64 & 29 & 5.56 & 5.57 & 46 & 5.47 & 5.61 \\
13 & 5.77 & 5.63 & 30 & 5.51 & 5.62 & 47 & 5.50 & 5.74 \\
14 & 5.75 & 5.61 & 31 & 5.57 & 5.55 & 48 & 5.38 & 5.65 \\
15 & 5.70 & 5.65 & 32 & 5.51 & 5.59 & 49 & 5.24 & 5.62 \\
16 & 5.70 & 5.57 & 33 & 5.57 & 5.56 & 50 & 5.31 & 5.63 \\
17 & 5.73 & 5.63 & 34 & 5.48 & 5.60 & & & \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Vanilla RNN with Attention: All 50 Epochs}
\label{subsec:vanilla-attn-complete}

\begin{table}[htbp]
\centering
\caption{Complete training log: Vanilla RNN with attention}
\label{tab:vanilla-attn-complete}
\small
\begin{tabular}{ccc|ccc|ccc}
\toprule
\textbf{Ep} & \textbf{Train} & \textbf{Val} & \textbf{Ep} & \textbf{Train} & \textbf{Val} & \textbf{Ep} & \textbf{Train} & \textbf{Val} \\
\midrule
1 & 16.93 & 7.90 & 18 & 6.94 & 7.06 & 35 & 3.81 & 5.13 \\
2 & 8.09 & 7.29 & 19 & 6.90 & 7.09 & 36 & 3.71 & 4.73 \\
3 & 7.68 & 7.09 & 20 & 6.79 & 6.78 & 37 & 3.69 & 5.09 \\
4 & 7.69 & 7.18 & 21 & 6.91 & 6.49 & 38 & 3.64 & 4.91 \\
5 & 7.67 & 7.19 & 22 & 6.89 & 6.74 & 39 & 3.58 & 4.99 \\
6 & 7.84 & 7.09 & 23 & 6.43 & 11.89 & 40 & 3.56 & 4.58 \\
7 & 7.57 & 7.03 & 24 & 9.97 & 10.88 & 41 & 3.66 & 5.01 \\
8 & 7.56 & 7.04 & 25 & 7.16 & 5.35 & 42 & 3.68 & 4.82 \\
9 & 7.58 & 7.15 & 26 & 4.98 & 4.99 & 43 & 3.50 & 4.75 \\
10 & 7.36 & 7.09 & 27 & 4.54 & 5.44 & 44 & 3.52 & 4.64 \\
11 & 7.38 & 7.16 & 28 & 4.16 & 5.67 & 45 & 3.47 & 4.53 \\
12 & 7.46 & 7.04 & 29 & 4.07 & 5.41 & 46 & 3.43 & 4.27 \\
13 & 7.29 & 6.97 & 30 & 3.96 & 5.45 & 47 & 3.41 & 4.31 \\
14 & 7.26 & 6.76 & 31 & 3.92 & 5.34 & 48 & 3.55 & 4.61 \\
15 & 7.06 & 6.99 & 32 & 3.90 & 5.36 & 49 & 3.44 & 4.42 \\
16 & 6.91 & 7.35 & 33 & 3.94 & 5.10 & 50 & 3.42 & 4.48 \\
17 & 6.89 & 6.77 & 34 & 3.84 & 4.77 & & & \\
\bottomrule
\end{tabular}
\end{table}

\section{Summary}
\label{sec:results-summary}

The experimental results demonstrate that the Bahdanau attention mechanism provides substantial performance improvements over the baseline Vanilla RNN architecture. The attention-augmented model achieved 13$\times$ better accuracy, 85.9\% lower CTC loss, and dramatically reduced perplexity. Both models exhibited strong learning dynamics, with the attention model showing continued improvement throughout all 50 epochs, while the baseline model plateaued after epoch 10.
